Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 16, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 16
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 8, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1958511287  0.1955990220  0.1812366738  0.1816239316  0.2193113772  0.2005988024  0.0512086514  0.0496815287  0.0000000000  2.2696068287  0.6019635201  0             67.999235868 
0.9420378279  0.8655256724  0.9552238806  0.9123931624  0.9872754491  0.9401197605  0.5238549618  0.5414012739  1.7964071856  0.3340990233  0.7000021935  300           0.1638520122 
0.8529591214  0.7872860636  0.9253731343  0.8760683761  0.9648203593  0.8952095808  0.6599872774  0.6420382166  3.5928143713  0.0919913825  0.7000021935  600           0.1718888871 
0.9926784625  0.9193154034  0.9866737740  0.9273504274  0.9977544910  0.9580838323  0.6020992366  0.6000000000  5.3892215569  0.0472272644  0.7000021935  900           0.1695434634 
0.9969493594  0.9144254279  0.9941364606  0.9294871795  0.9985029940  0.9520958084  0.6001908397  0.5808917197  7.1856287425  0.0390688198  0.7000021935  1200          0.1705973148 
1.0000000000  0.9339853301  0.9989339019  0.9465811966  0.9992514970  0.9550898204  0.7076972010  0.7121019108  8.9820359281  0.0112727781  0.7000021935  1500          0.1711558199 
0.9963392312  0.9315403423  0.9973347548  0.9465811966  0.9940119760  0.9401197605  0.6943384224  0.6840764331  10.778443113  0.0264132636  0.7000021935  1800          0.1724889247 
1.0000000000  0.9290953545  1.0000000000  0.9487179487  1.0000000000  0.9640718563  0.6466284987  0.6484076433  12.574850299  0.0099450280  0.7000021935  2100          0.1715048893 
1.0000000000  0.9217603912  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6504452926  0.6292993631  14.371257485  0.0005295032  0.7000021935  2400          0.1739948972 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6609414758  0.6471337580  16.167664670  0.0000556398  0.7000021935  2700          0.1818673595 
1.0000000000  0.9242053790  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6666666667  0.6547770701  17.964071856  0.0000389793  0.7000021935  3000          0.1891542411 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6676208651  0.6560509554  19.760479041  0.0000213937  0.7000021935  3300          0.1866005357 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6695292621  0.6560509554  21.556886227  0.0000174875  0.7000021935  3600          0.1935759179 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6698473282  0.6560509554  23.353293413  0.0000131708  0.7000021935  3900          0.1786705589 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6736641221  0.6573248408  25.149700598  0.0000104120  0.7000021935  4200          0.1800832272 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9700598802  0.6708015267  0.6598726115  26.946107784  0.0000081443  0.7000021935  4500          0.1896550449 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6746183206  0.6636942675  28.742514970  0.0000060487  0.7000021935  4800          0.1785616223 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6743002545  0.6636942675  29.940119760  0.0000055475  0.7000021935  5000          0.1870605004
Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/PACS_edge/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 8, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1561928005  0.1149144254  0.1823027719  0.1495726496  0.1339820359  0.1437125749  0.1253180662  0.1095541401  0.0000000000  2.0800325871  0.6019635201  0             37.917980909 
0.7291031117  0.6283618582  0.8001066098  0.7264957265  0.8300898204  0.7185628743  0.5677480916  0.6025477707  1.7964071856  0.8065385808  0.7000021935  300           0.1571933174 
0.8852959121  0.7114914425  0.9035181237  0.8205128205  0.9491017964  0.7964071856  0.7331424936  0.7528662420  3.5928143713  0.3192377030  0.7000021935  600           0.1658597747 
0.9725442343  0.7677261614  0.9813432836  0.8547008547  0.9895209581  0.8113772455  0.7194656489  0.7477707006  5.3892215569  0.1366438002  0.7000021935  900           0.1657733377 
0.9865771812  0.7530562347  0.9893390192  0.8632478632  0.9947604790  0.8323353293  0.7725826972  0.7872611465  7.1856287425  0.0813669820  0.7000021935  1200          0.1668254526 
0.9847467968  0.7457212714  0.9861407249  0.8632478632  0.9917664671  0.8383233533  0.7363231552  0.7452229299  8.9820359281  0.0447957555  0.7000021935  1500          0.1673945864 
0.9902379500  0.7555012225  0.9888059701  0.8632478632  0.9932634731  0.8053892216  0.7048346056  0.7337579618  10.778443113  0.0296375043  0.7000021935  1800          0.1663131873 
0.9713239780  0.7432762836  0.9818763326  0.8226495726  0.9872754491  0.8023952096  0.7070610687  0.7388535032  12.574850299  0.0306497706  0.7000021935  2100          0.1660725387 
0.9969493594  0.7555012225  0.9989339019  0.8952991453  0.9985029940  0.7994011976  0.6701653944  0.7082802548  14.371257485  0.0151573246  0.7000021935  2400          0.1663182886 
0.9969493594  0.7677261614  0.9973347548  0.8696581197  0.9985029940  0.8053892216  0.7236005089  0.7350318471  16.167664670  0.0133473229  0.7000021935  2700          0.1662508051 
0.9530201342  0.7310513447  0.9760127932  0.8440170940  0.9648203593  0.7694610778  0.6765267176  0.6929936306  17.964071856  0.0256389209  0.7000021935  3000          0.1664226055 
0.9871873093  0.7383863081  0.9978678038  0.8760683761  0.9985029940  0.8413173653  0.7767175573  0.7745222930  19.760479041  0.0216464534  0.7000021935  3300          0.1672974340 
0.9969493594  0.7823960880  0.9936034115  0.8632478632  0.9992514970  0.8263473054  0.7439567430  0.7783439490  21.556886227  0.0502244984  0.7000021935  3600          0.1666798536 
0.9993898719  0.7897310513  0.9989339019  0.8803418803  1.0000000000  0.8443113772  0.7045165394  0.7146496815  23.353293413  0.0152758006  0.7000021935  3900          0.1667281397 
1.0000000000  0.7897310513  0.9989339019  0.8846153846  1.0000000000  0.8473053892  0.7538167939  0.7732484076  25.149700598  0.0111851983  0.7000021935  4200          0.1663402303 
0.9987797437  0.7775061125  1.0000000000  0.8995726496  1.0000000000  0.8622754491  0.7344147583  0.7515923567  26.946107784  0.0030829371  0.7000021935  4500          0.1669395852 
0.9993898719  0.7726161369  0.9984008529  0.8717948718  0.9977544910  0.8383233533  0.7175572519  0.7439490446  28.742514970  0.0349552446  0.7000021935  4800          0.1670193585 
0.9987797437  0.7603911980  0.9989339019  0.8589743590  0.9992514970  0.8712574850  0.7105597964  0.7414012739  29.940119760  0.0115524197  0.7000021935  5000          0.1653976774 


Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1671751068  0.1393643032  0.2382729211  0.2628205128  0.2612275449  0.2544910180  0.2286895674  0.2140127389  0.0000000000  2.0756614208  2.2035861015  0             2.9190127850 
0.7705918243  0.7799511002  0.9904051173  0.9444444444  0.9940119760  0.9401197605  0.9697837150  0.9197452229  7.1856287425  0.2175522910  2.3044762611  300           0.4828616118 
0.7687614399  0.7677261614  0.9994669510  0.9487179487  1.0000000000  0.9580838323  0.9952290076  0.9388535032  14.371257485  0.0220809113  2.3044762611  600           0.4996907441 
0.7913361806  0.7701711491  1.0000000000  0.9529914530  1.0000000000  0.9610778443  0.9984096692  0.9388535032  21.556886227  0.0059579228  2.3044762611  900           0.4900596484 
0.7492373398  0.7701711491  0.9952025586  0.9508547009  0.9992514970  0.9520958084  0.9936386768  0.9248407643  28.742514970  0.0059719805  2.3044762611  1200          0.4831658816 
0.7766931056  0.7872860636  1.0000000000  0.9636752137  1.0000000000  0.9341317365  0.9996819338  0.9388535032  35.928143712  0.0094478442  2.3044762611  1500          0.5031758062 
0.8010982306  0.7701711491  1.0000000000  0.9700854701  1.0000000000  0.9431137725  1.0000000000  0.9414012739  43.113772455  0.0034129516  2.3044762611  1800          0.4906942487 
0.7998779744  0.7921760391  1.0000000000  0.9572649573  1.0000000000  0.9580838323  1.0000000000  0.9439490446  50.299401197  0.0000461318  2.3044762611  2100          0.4903329555 
0.8004881025  0.7921760391  1.0000000000  0.9572649573  1.0000000000  0.9610778443  1.0000000000  0.9452229299  57.485029940  0.0000183598  2.3044762611  2400          0.4814332191 
0.7992678462  0.7946210269  1.0000000000  0.9572649573  1.0000000000  0.9640718563  1.0000000000  0.9464968153  64.670658682  0.0000122645  2.3044762611  2700          0.4808498240 
0.7956070775  0.7921760391  1.0000000000  0.9572649573  1.0000000000  0.9640718563  1.0000000000  0.9464968153  71.856287425  0.0000083771  2.3044762611  3000          0.4751661110 
0.7968273337  0.7921760391  1.0000000000  0.9572649573  1.0000000000  0.9670658683  1.0000000000  0.9477707006  79.041916167  0.0000060934  2.3044762611  3300          0.4745226725 
0.7980475900  0.7897310513  1.0000000000  0.9572649573  1.0000000000  0.9670658683  1.0000000000  0.9477707006  86.227544910  0.0000049638  2.3044762611  3600          0.4866823490 
0.7956070775  0.7946210269  1.0000000000  0.9551282051  1.0000000000  0.9670658683  1.0000000000  0.9477707006  93.413173652  0.0000037869  2.3044762611  3900          0.4779215407 
0.7943868212  0.7946210269  1.0000000000  0.9572649573  1.0000000000  0.9640718563  1.0000000000  0.9477707006  100.59880239  0.0000027957  2.3044762611  4200          0.4795120517 
0.7943868212  0.7946210269  1.0000000000  0.9551282051  1.0000000000  0.9610778443  1.0000000000  0.9464968153  107.78443113  0.0000024077  2.3044762611  4500          0.4820281943 
0.7956070775  0.7946210269  1.0000000000  0.9551282051  1.0000000000  0.9640718563  1.0000000000  0.9490445860  114.97005988  0.0000019645  2.3044762611  4800          0.4787527919 
0.7931665650  0.7946210269  1.0000000000  0.9551282051  1.0000000000  0.9670658683  1.0000000000  0.9503184713  119.76047904  0.0000017086  2.3044762611  5000          0.4870604837 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1903599756  0.1564792176  0.2222814499  0.2564102564  0.2597305389  0.2604790419  0.2337786260  0.2152866242  0.0000000000  2.0947883129  2.2035861015  0             1.4948267937 
0.9938987187  0.9046454768  0.6812366738  0.6858974359  1.0000000000  0.9491017964  0.9777353690  0.9146496815  7.1856287425  0.2295345472  2.3044762611  300           0.4912793851 
1.0000000000  0.9070904645  0.6636460554  0.6452991453  1.0000000000  0.9520958084  0.9955470738  0.9299363057  14.371257485  0.0207941741  2.3044762611  600           0.4964570697 
0.9963392312  0.9168704156  0.6375266525  0.6452991453  0.9970059880  0.9371257485  0.9993638677  0.9350318471  21.556886227  0.0031864139  2.3044762611  900           0.4908864562 
1.0000000000  0.9070904645  0.6769722814  0.6816239316  1.0000000000  0.9401197605  1.0000000000  0.9324840764  28.742514970  0.0087585596  2.3044762611  1200          0.4946958510 
1.0000000000  0.9070904645  0.6721748401  0.6709401709  1.0000000000  0.9580838323  0.9993638677  0.9337579618  35.928143712  0.0110681548  2.3044762611  1500          0.5000253423 
1.0000000000  0.9193154034  0.6844349680  0.6923076923  1.0000000000  0.9520958084  1.0000000000  0.9401273885  43.113772455  0.0081237265  2.3044762611  1800          0.4887076219 
1.0000000000  0.9217603912  0.6956289979  0.7029914530  1.0000000000  0.9491017964  1.0000000000  0.9464968153  50.299401197  0.0037212512  2.3044762611  2100          0.4876854380 
1.0000000000  0.9242053790  0.6950959488  0.7029914530  1.0000000000  0.9520958084  1.0000000000  0.9515923567  57.485029940  0.0000770893  2.3044762611  2400          0.4899529775 
1.0000000000  0.9217603912  0.6950959488  0.7115384615  1.0000000000  0.9491017964  1.0000000000  0.9515923567  64.670658682  0.0000155532  2.3044762611  2700          0.4902841258 
1.0000000000  0.9217603912  0.6945628998  0.7072649573  1.0000000000  0.9491017964  1.0000000000  0.9528662420  71.856287425  0.0000104344  2.3044762611  3000          0.5023188368 
1.0000000000  0.9217603912  0.6945628998  0.7094017094  1.0000000000  0.9491017964  1.0000000000  0.9528662420  79.041916167  0.0000074634  2.3044762611  3300          0.4937963804 
1.0000000000  0.9217603912  0.6950959488  0.7072649573  1.0000000000  0.9520958084  1.0000000000  0.9515923567  86.227544910  0.0000056498  2.3044762611  3600          0.4913214970 
1.0000000000  0.9242053790  0.6945628998  0.7115384615  1.0000000000  0.9520958084  1.0000000000  0.9528662420  93.413173652  0.0000043753  2.3044762611  3900          0.5002845931 
1.0000000000  0.9242053790  0.6961620469  0.7072649573  1.0000000000  0.9520958084  1.0000000000  0.9515923567  100.59880239  0.0000033294  2.3044762611  4200          0.4908134818 
1.0000000000  0.9242053790  0.6961620469  0.7136752137  1.0000000000  0.9520958084  1.0000000000  0.9528662420  107.78443113  0.0000026748  2.3044762611  4500          0.5061428531 
1.0000000000  0.9266503667  0.6945628998  0.7094017094  1.0000000000  0.9520958084  1.0000000000  0.9503184713  114.97005988  0.0000020366  2.3044762611  4800          0.4968081403 
1.0000000000  0.9266503667  0.6940298507  0.7136752137  1.0000000000  0.9520958084  1.0000000000  0.9528662420  119.76047904  0.0000018921  2.3044762611  5000          0.4979979861 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1604636974  0.1491442543  0.2009594883  0.1880341880  0.0920658683  0.1317365269  0.2216921120  0.2089171975  0.0000000000  2.0450930595  2.2035861015  0             1.9521110058 
0.9987797437  0.8997555012  0.9952025586  0.9444444444  0.9461077844  0.9251497006  0.9701017812  0.9146496815  7.1856287425  0.2805486548  2.3044762611  300           0.4923729960 
1.0000000000  0.9095354523  0.9989339019  0.9423076923  0.9423652695  0.9041916168  0.9965012723  0.9248407643  14.371257485  0.0259454110  2.3044762611  600           0.4883697859 
1.0000000000  0.9144254279  1.0000000000  0.9508547009  0.9386227545  0.8982035928  1.0000000000  0.9388535032  21.556886227  0.0055603403  2.3044762611  900           0.4893748395 
1.0000000000  0.9193154034  1.0000000000  0.9487179487  0.9333832335  0.9071856287  1.0000000000  0.9363057325  28.742514970  0.0001576587  2.3044762611  1200          0.4854061174 
1.0000000000  0.9144254279  1.0000000000  0.9508547009  0.9296407186  0.9011976048  1.0000000000  0.9337579618  35.928143712  0.0000545381  2.3044762611  1500          0.4815405210 
1.0000000000  0.9168704156  1.0000000000  0.9508547009  0.9281437126  0.9011976048  1.0000000000  0.9337579618  43.113772455  0.0000319352  2.3044762611  1800          0.4893488272 
1.0000000000  0.9119804401  1.0000000000  0.9551282051  0.9303892216  0.9041916168  1.0000000000  0.9375796178  50.299401197  0.0000210770  2.3044762611  2100          0.4853299220 
1.0000000000  0.9168704156  1.0000000000  0.9572649573  0.9288922156  0.9011976048  1.0000000000  0.9388535032  57.485029940  0.0000146018  2.3044762611  2400          0.4860653941 
1.0000000000  0.9095354523  1.0000000000  0.9529914530  0.9281437126  0.9011976048  1.0000000000  0.9337579618  64.670658682  0.0000120760  2.3044762611  2700          0.4899518514 
1.0000000000  0.9119804401  1.0000000000  0.9551282051  0.9273952096  0.9011976048  1.0000000000  0.9337579618  71.856287425  0.0000089139  2.3044762611  3000          0.4807687171 
1.0000000000  0.9095354523  1.0000000000  0.9551282051  0.9266467066  0.9011976048  1.0000000000  0.9350318471  79.041916167  0.0000073871  2.3044762611  3300          0.4811836783 
1.0000000000  0.9119804401  1.0000000000  0.9529914530  0.9273952096  0.9011976048  1.0000000000  0.9350318471  86.227544910  0.0000059288  2.3044762611  3600          0.4877692453 
1.0000000000  0.9095354523  1.0000000000  0.9529914530  0.9266467066  0.9011976048  1.0000000000  0.9350318471  93.413173652  0.0000046551  2.3044762611  3900          0.4888900185 
1.0000000000  0.9095354523  1.0000000000  0.9572649573  0.9244011976  0.9011976048  1.0000000000  0.9363057325  100.59880239  0.0000037419  2.3044762611  4200          0.4832633233 
1.0000000000  0.9144254279  1.0000000000  0.9572649573  0.9244011976  0.9041916168  1.0000000000  0.9363057325  107.78443113  0.0000032382  2.3044762611  4500          0.4942519363 
1.0000000000  0.9095354523  1.0000000000  0.9551282051  0.9244011976  0.9041916168  1.0000000000  0.9363057325  114.97005988  0.0000026275  2.3044762611  4800          0.4839172713 
1.0000000000  0.9095354523  1.0000000000  0.9551282051  0.9244011976  0.9041916168  1.0000000000  0.9350318471  119.76047904  0.0000021625  2.3044762611  5000          0.4893596137 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.2050030506  0.1784841076  0.2158848614  0.2094017094  0.2305389222  0.2544910180  0.2000636132  0.2038216561  0.0000000000  2.1355340481  2.2035861015  0             3.9524939060 
0.9969493594  0.9070904645  0.9968017058  0.9273504274  1.0000000000  0.9371257485  0.6323155216  0.6191082803  7.1856287425  0.1737604117  2.3044762611  300           0.4963676095 
1.0000000000  0.9168704156  1.0000000000  0.9337606838  1.0000000000  0.9401197605  0.6434478372  0.6458598726  14.371257485  0.0045451088  2.3044762611  600           0.4910226822 
1.0000000000  0.9193154034  1.0000000000  0.9423076923  1.0000000000  0.9431137725  0.6812977099  0.6751592357  21.556886227  0.0001809691  2.3044762611  900           0.4886733143 
1.0000000000  0.9144254279  1.0000000000  0.9380341880  1.0000000000  0.9431137725  0.6844783715  0.6751592357  28.742514970  0.0000411767  2.3044762611  1200          0.4895279535 
1.0000000000  0.9168704156  1.0000000000  0.9423076923  1.0000000000  0.9431137725  0.6844783715  0.6738853503  35.928143712  0.0000241072  2.3044762611  1500          0.4917118438 
1.0000000000  0.9217603912  1.0000000000  0.9423076923  1.0000000000  0.9431137725  0.6851145038  0.6764331210  43.113772455  0.0000159733  2.3044762611  1800          0.4923332548 
1.0000000000  0.9168704156  1.0000000000  0.9423076923  1.0000000000  0.9431137725  0.6832061069  0.6764331210  50.299401197  0.0000109917  2.3044762611  2100          0.4964071512 
1.0000000000  0.9144254279  1.0000000000  0.9423076923  1.0000000000  0.9431137725  0.6835241730  0.6751592357  57.485029940  0.0000077720  2.3044762611  2400          0.4892376947 
1.0000000000  0.9168704156  1.0000000000  0.9444444444  1.0000000000  0.9431137725  0.6787531807  0.6726114650  64.670658682  0.0000062893  2.3044762611  2700          0.4906000288 
1.0000000000  0.9168704156  1.0000000000  0.9465811966  1.0000000000  0.9431137725  0.6777989822  0.6751592357  71.856287425  0.0000046141  2.3044762611  3000          0.4977640367 
1.0000000000  0.9168704156  1.0000000000  0.9444444444  1.0000000000  0.9431137725  0.6777989822  0.6726114650  79.041916167  0.0000037848  2.3044762611  3300          0.4866084210 
1.0000000000  0.9168704156  1.0000000000  0.9465811966  1.0000000000  0.9431137725  0.6790712468  0.6726114650  86.227544910  0.0000029581  2.3044762611  3600          0.4884964244 
1.0000000000  0.9144254279  1.0000000000  0.9444444444  1.0000000000  0.9431137725  0.6777989822  0.6713375796  93.413173652  0.0000023562  2.3044762611  3900          0.4977837976 

