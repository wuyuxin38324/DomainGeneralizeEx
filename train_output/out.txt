Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 16, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 16
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 8, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1958511287  0.1955990220  0.1812366738  0.1816239316  0.2193113772  0.2005988024  0.0512086514  0.0496815287  0.0000000000  2.2696068287  0.6019635201  0             67.999235868 
0.9420378279  0.8655256724  0.9552238806  0.9123931624  0.9872754491  0.9401197605  0.5238549618  0.5414012739  1.7964071856  0.3340990233  0.7000021935  300           0.1638520122 
0.8529591214  0.7872860636  0.9253731343  0.8760683761  0.9648203593  0.8952095808  0.6599872774  0.6420382166  3.5928143713  0.0919913825  0.7000021935  600           0.1718888871 
0.9926784625  0.9193154034  0.9866737740  0.9273504274  0.9977544910  0.9580838323  0.6020992366  0.6000000000  5.3892215569  0.0472272644  0.7000021935  900           0.1695434634 
0.9969493594  0.9144254279  0.9941364606  0.9294871795  0.9985029940  0.9520958084  0.6001908397  0.5808917197  7.1856287425  0.0390688198  0.7000021935  1200          0.1705973148 
1.0000000000  0.9339853301  0.9989339019  0.9465811966  0.9992514970  0.9550898204  0.7076972010  0.7121019108  8.9820359281  0.0112727781  0.7000021935  1500          0.1711558199 
0.9963392312  0.9315403423  0.9973347548  0.9465811966  0.9940119760  0.9401197605  0.6943384224  0.6840764331  10.778443113  0.0264132636  0.7000021935  1800          0.1724889247 
1.0000000000  0.9290953545  1.0000000000  0.9487179487  1.0000000000  0.9640718563  0.6466284987  0.6484076433  12.574850299  0.0099450280  0.7000021935  2100          0.1715048893 
1.0000000000  0.9217603912  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6504452926  0.6292993631  14.371257485  0.0005295032  0.7000021935  2400          0.1739948972 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6609414758  0.6471337580  16.167664670  0.0000556398  0.7000021935  2700          0.1818673595 
1.0000000000  0.9242053790  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6666666667  0.6547770701  17.964071856  0.0000389793  0.7000021935  3000          0.1891542411 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6676208651  0.6560509554  19.760479041  0.0000213937  0.7000021935  3300          0.1866005357 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6695292621  0.6560509554  21.556886227  0.0000174875  0.7000021935  3600          0.1935759179 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9730538922  0.6698473282  0.6560509554  23.353293413  0.0000131708  0.7000021935  3900          0.1786705589 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6736641221  0.6573248408  25.149700598  0.0000104120  0.7000021935  4200          0.1800832272 
1.0000000000  0.9266503667  1.0000000000  0.9508547009  1.0000000000  0.9700598802  0.6708015267  0.6598726115  26.946107784  0.0000081443  0.7000021935  4500          0.1896550449 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6746183206  0.6636942675  28.742514970  0.0000060487  0.7000021935  4800          0.1785616223 
1.0000000000  0.9266503667  1.0000000000  0.9529914530  1.0000000000  0.9730538922  0.6743002545  0.6636942675  29.940119760  0.0000055475  0.7000021935  5000          0.1870605004
Environment:
	Python: 3.10.12
	PyTorch: 1.12.1
	Torchvision: 0.13.1
	CUDA: 11.3
	CUDNN: 8302
	NumPy: 1.26.0
	PIL: 10.0.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/PACS_edge/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 8, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 8
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
D:\Users\wuyux\anaconda3\envs\domainbed\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1561928005  0.1149144254  0.1823027719  0.1495726496  0.1339820359  0.1437125749  0.1253180662  0.1095541401  0.0000000000  2.0800325871  0.6019635201  0             37.917980909 
0.7291031117  0.6283618582  0.8001066098  0.7264957265  0.8300898204  0.7185628743  0.5677480916  0.6025477707  1.7964071856  0.8065385808  0.7000021935  300           0.1571933174 
0.8852959121  0.7114914425  0.9035181237  0.8205128205  0.9491017964  0.7964071856  0.7331424936  0.7528662420  3.5928143713  0.3192377030  0.7000021935  600           0.1658597747 
0.9725442343  0.7677261614  0.9813432836  0.8547008547  0.9895209581  0.8113772455  0.7194656489  0.7477707006  5.3892215569  0.1366438002  0.7000021935  900           0.1657733377 
0.9865771812  0.7530562347  0.9893390192  0.8632478632  0.9947604790  0.8323353293  0.7725826972  0.7872611465  7.1856287425  0.0813669820  0.7000021935  1200          0.1668254526 
0.9847467968  0.7457212714  0.9861407249  0.8632478632  0.9917664671  0.8383233533  0.7363231552  0.7452229299  8.9820359281  0.0447957555  0.7000021935  1500          0.1673945864 
0.9902379500  0.7555012225  0.9888059701  0.8632478632  0.9932634731  0.8053892216  0.7048346056  0.7337579618  10.778443113  0.0296375043  0.7000021935  1800          0.1663131873 
0.9713239780  0.7432762836  0.9818763326  0.8226495726  0.9872754491  0.8023952096  0.7070610687  0.7388535032  12.574850299  0.0306497706  0.7000021935  2100          0.1660725387 
0.9969493594  0.7555012225  0.9989339019  0.8952991453  0.9985029940  0.7994011976  0.6701653944  0.7082802548  14.371257485  0.0151573246  0.7000021935  2400          0.1663182886 
0.9969493594  0.7677261614  0.9973347548  0.8696581197  0.9985029940  0.8053892216  0.7236005089  0.7350318471  16.167664670  0.0133473229  0.7000021935  2700          0.1662508051 
0.9530201342  0.7310513447  0.9760127932  0.8440170940  0.9648203593  0.7694610778  0.6765267176  0.6929936306  17.964071856  0.0256389209  0.7000021935  3000          0.1664226055 
0.9871873093  0.7383863081  0.9978678038  0.8760683761  0.9985029940  0.8413173653  0.7767175573  0.7745222930  19.760479041  0.0216464534  0.7000021935  3300          0.1672974340 
0.9969493594  0.7823960880  0.9936034115  0.8632478632  0.9992514970  0.8263473054  0.7439567430  0.7783439490  21.556886227  0.0502244984  0.7000021935  3600          0.1666798536 
0.9993898719  0.7897310513  0.9989339019  0.8803418803  1.0000000000  0.8443113772  0.7045165394  0.7146496815  23.353293413  0.0152758006  0.7000021935  3900          0.1667281397 
1.0000000000  0.7897310513  0.9989339019  0.8846153846  1.0000000000  0.8473053892  0.7538167939  0.7732484076  25.149700598  0.0111851983  0.7000021935  4200          0.1663402303 
0.9987797437  0.7775061125  1.0000000000  0.8995726496  1.0000000000  0.8622754491  0.7344147583  0.7515923567  26.946107784  0.0030829371  0.7000021935  4500          0.1669395852 
0.9993898719  0.7726161369  0.9984008529  0.8717948718  0.9977544910  0.8383233533  0.7175572519  0.7439490446  28.742514970  0.0349552446  0.7000021935  4800          0.1670193585 
0.9987797437  0.7603911980  0.9989339019  0.8589743590  0.9992514970  0.8712574850  0.7105597964  0.7414012739  29.940119760  0.0115524197  0.7000021935  5000          0.1653976774 
