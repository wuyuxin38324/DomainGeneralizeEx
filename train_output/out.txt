Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
  0%|          | 0.00/44.7M [00:00<?, ?B/s]  1%|1         | 496k/44.7M [00:00<00:09, 5.04MB/s]  4%|4         | 1.83M/44.7M [00:00<00:04, 9.97MB/s]  7%|6         | 3.02M/44.7M [00:00<00:04, 10.9MB/s] 10%|9         | 4.39M/44.7M [00:00<00:03, 11.8MB/s] 13%|#2        | 5.78M/44.7M [00:00<00:03, 12.6MB/s] 16%|#6        | 7.24M/44.7M [00:00<00:02, 13.3MB/s] 21%|##        | 9.23M/44.7M [00:00<00:02, 15.3MB/s] 28%|##7       | 12.5M/44.7M [00:00<00:01, 21.0MB/s] 33%|###2      | 14.6M/44.7M [00:00<00:01, 19.8MB/s] 41%|####      | 18.2M/44.7M [00:01<00:01, 25.0MB/s] 46%|####6     | 20.6M/44.7M [00:01<00:01, 25.0MB/s] 52%|#####1    | 23.1M/44.7M [00:01<00:00, 24.9MB/s] 58%|#####8    | 26.1M/44.7M [00:01<00:00, 26.3MB/s] 68%|######7   | 30.3M/44.7M [00:01<00:00, 31.0MB/s] 84%|########4 | 37.7M/44.7M [00:01<00:00, 44.3MB/s]100%|##########| 44.7M/44.7M [00:01<00:00, 28.2MB/s]
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1708358755  0.1638141809  0.2627931770  0.2970085470  0.2350299401  0.2425149701  0.2369592875  0.2242038217  0.0000000000  2.0843756199  2.2035861015  0             1.8065040112 
0.7553386211  0.7555012225  0.9914712154  0.9166666667  0.9977544910  0.9491017964  0.9821882952  0.9261146497  7.1856287425  0.2096718786  2.3044762611  300           0.3692024970 
0.7657107993  0.7677261614  1.0000000000  0.9529914530  1.0000000000  0.9580838323  0.9965012723  0.9414012739  14.371257485  0.0195924636  2.3044762611  600           0.3727727683 
0.6943258084  0.7139364303  0.9984008529  0.9273504274  1.0000000000  0.9610778443  0.9974554707  0.9222929936  21.556886227  0.0078943415  2.3044762611  900           0.3753283175 
0.7266625991  0.7603911980  0.9989339019  0.9551282051  1.0000000000  0.9550898204  0.9990458015  0.9350318471  28.742514970  0.0065765794  2.3044762611  1200          0.3782802987 
0.7425259304  0.7726161369  1.0000000000  0.9572649573  1.0000000000  0.9610778443  1.0000000000  0.9464968153  35.928143712  0.0005489867  2.3044762611  1500          0.3745560042 
0.7486272117  0.7677261614  1.0000000000  0.9594017094  1.0000000000  0.9580838323  1.0000000000  0.9477707006  43.113772455  0.0001535091  2.3044762611  1800          0.3730426852 
0.7504575961  0.7628361858  1.0000000000  0.9594017094  1.0000000000  0.9610778443  1.0000000000  0.9464968153  50.299401197  0.0000310955  2.3044762611  2100          0.3799124432 
0.7504575961  0.7652811736  1.0000000000  0.9615384615  1.0000000000  0.9610778443  1.0000000000  0.9464968153  57.485029940  0.0000122059  2.3044762611  2400          0.3820674022 
0.7486272117  0.7652811736  1.0000000000  0.9572649573  1.0000000000  0.9610778443  1.0000000000  0.9464968153  64.670658682  0.0000092386  2.3044762611  2700          0.3717977262 
0.7474069555  0.7628361858  1.0000000000  0.9572649573  1.0000000000  0.9610778443  1.0000000000  0.9464968153  71.856287425  0.0000064043  2.3044762611  3000          0.3809643658 
0.7474069555  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9610778443  1.0000000000  0.9452229299  79.041916167  0.0000052165  2.3044762611  3300          0.3719895601 
0.7474069555  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9610778443  1.0000000000  0.9464968153  86.227544910  0.0000041297  2.3044762611  3600          0.3758189034 
0.7474069555  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9580838323  1.0000000000  0.9439490446  93.413173652  0.0000032290  2.3044762611  3900          0.3789280200 
0.7467968273  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9580838323  1.0000000000  0.9464968153  100.59880239  0.0000026086  2.3044762611  4200          0.3735930053 
0.7443563148  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9580838323  1.0000000000  0.9477707006  107.78443113  0.0000021436  2.3044762611  4500          0.3746030593 
0.7443563148  0.7628361858  1.0000000000  0.9551282051  1.0000000000  0.9610778443  1.0000000000  0.9452229299  114.97005988  0.0000018072  2.3044762611  4800          0.3822449327 
0.7425259304  0.7628361858  1.0000000000  0.9572649573  1.0000000000  0.9580838323  1.0000000000  0.9452229299  119.76047904  0.0000015251  2.3044762611  5000          0.3698848629 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1934106162  0.1955990220  0.2340085288  0.2692307692  0.2425149701  0.2425149701  0.2350508906  0.2280254777  0.0000000000  2.1016135216  2.2035861015  0             1.3870522976 
1.0000000000  0.9168704156  0.6764392324  0.6752136752  1.0000000000  0.9640718563  0.9710559796  0.9171974522  7.1856287425  0.2073764658  2.3044762611  300           0.3728441509 
0.9993898719  0.9290953545  0.6679104478  0.6623931624  1.0000000000  0.9730538922  0.9961832061  0.9261146497  14.371257485  0.0235311285  2.3044762611  600           0.3758640925 
0.9993898719  0.9242053790  0.6759061834  0.6688034188  1.0000000000  0.9550898204  0.9984096692  0.9248407643  21.556886227  0.0042448834  2.3044762611  900           0.3733535695 
0.9993898719  0.9217603912  0.6982942431  0.7029914530  1.0000000000  0.9580838323  0.9996819338  0.9414012739  28.742514970  0.0108674562  2.3044762611  1200          0.3733478419 
1.0000000000  0.9242053790  0.6812366738  0.6752136752  1.0000000000  0.9700598802  1.0000000000  0.9312101911  35.928143712  0.0057982421  2.3044762611  1500          0.3726458375 
1.0000000000  0.9290953545  0.7062899787  0.7029914530  1.0000000000  0.9730538922  1.0000000000  0.9388535032  43.113772455  0.0001082829  2.3044762611  1800          0.3755962531 
1.0000000000  0.9339853301  0.7046908316  0.7029914530  1.0000000000  0.9730538922  1.0000000000  0.9414012739  50.299401197  0.0000235004  2.3044762611  2100          0.3734783983 
1.0000000000  0.9315403423  0.7078891258  0.7051282051  1.0000000000  0.9700598802  1.0000000000  0.9439490446  57.485029940  0.0000138700  2.3044762611  2400          0.3709138155 
1.0000000000  0.9290953545  0.7068230277  0.7029914530  1.0000000000  0.9700598802  1.0000000000  0.9426751592  64.670658682  0.0000094533  2.3044762611  2700          0.3793768128 
1.0000000000  0.9290953545  0.7062899787  0.7008547009  1.0000000000  0.9700598802  1.0000000000  0.9426751592  71.856287425  0.0000066171  2.3044762611  3000          0.3731130632 
1.0000000000  0.9315403423  0.7030916844  0.6987179487  1.0000000000  0.9700598802  1.0000000000  0.9439490446  79.041916167  0.0000053179  2.3044762611  3300          0.3723980268 
1.0000000000  0.9290953545  0.7030916844  0.6965811966  1.0000000000  0.9700598802  1.0000000000  0.9439490446  86.227544910  0.0000039658  2.3044762611  3600          0.3849457796 
1.0000000000  0.9290953545  0.7025586354  0.6987179487  1.0000000000  0.9700598802  1.0000000000  0.9439490446  93.413173652  0.0000030171  2.3044762611  3900          0.3756637025 
1.0000000000  0.9315403423  0.7062899787  0.7008547009  1.0000000000  0.9700598802  1.0000000000  0.9439490446  100.59880239  0.0000024962  2.3044762611  4200          0.3716734258 
1.0000000000  0.9315403423  0.7052238806  0.7008547009  1.0000000000  0.9700598802  1.0000000000  0.9439490446  107.78443113  0.0000021702  2.3044762611  4500          0.3795950301 
1.0000000000  0.9315403423  0.7057569296  0.7029914530  1.0000000000  0.9700598802  1.0000000000  0.9439490446  114.97005988  0.0000017008  2.3044762611  4800          0.3708150911 
1.0000000000  0.9315403423  0.7052238806  0.6987179487  1.0000000000  0.9700598802  1.0000000000  0.9439490446  119.76047904  0.0000014772  2.3044762611  5000          0.3719168341 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1708358755  0.1760391198  0.2436034115  0.2521367521  0.0995508982  0.1257485030  0.2328244275  0.2191082803  0.0000000000  2.0596985817  2.2035861015  0             1.0826904774 
0.9963392312  0.9144254279  0.9946695096  0.9316239316  0.9221556886  0.8832335329  0.9716921120  0.9235668790  7.1856287425  0.2691671851  2.3044762611  300           0.3730878250 
1.0000000000  0.9388753056  0.9994669510  0.9188034188  0.9281437126  0.8832335329  0.9872773537  0.8904458599  14.371257485  0.0193515165  2.3044762611  600           0.3695656967 
1.0000000000  0.9242053790  1.0000000000  0.9529914530  0.9416167665  0.9221556886  1.0000000000  0.9363057325  21.556886227  0.0130502089  2.3044762611  900           0.3699167116 
1.0000000000  0.9266503667  1.0000000000  0.9594017094  0.9258982036  0.8922155689  0.9987277354  0.9286624204  28.742514970  0.0042186997  2.3044762611  1200          0.3718909319 
1.0000000000  0.9266503667  1.0000000000  0.9551282051  0.9356287425  0.9131736527  1.0000000000  0.9324840764  35.928143712  0.0004888214  2.3044762611  1500          0.3812635978 
1.0000000000  0.9290953545  1.0000000000  0.9615384615  0.9341317365  0.9161676647  1.0000000000  0.9324840764  43.113772455  0.0000420086  2.3044762611  1800          0.3715645830 
1.0000000000  0.9266503667  1.0000000000  0.9636752137  0.9341317365  0.9131736527  1.0000000000  0.9363057325  50.299401197  0.0000183419  2.3044762611  2100          0.3731816300 
1.0000000000  0.9290953545  1.0000000000  0.9658119658  0.9333832335  0.9131736527  1.0000000000  0.9337579618  57.485029940  0.0000112370  2.3044762611  2400          0.3807090410 
1.0000000000  0.9290953545  1.0000000000  0.9636752137  0.9341317365  0.9131736527  1.0000000000  0.9363057325  64.670658682  0.0000087272  2.3044762611  2700          0.3714335648 
1.0000000000  0.9290953545  1.0000000000  0.9636752137  0.9341317365  0.9131736527  1.0000000000  0.9350318471  71.856287425  0.0000064776  2.3044762611  3000          0.3754944523 
1.0000000000  0.9290953545  1.0000000000  0.9636752137  0.9341317365  0.9131736527  1.0000000000  0.9350318471  79.041916167  0.0000050469  2.3044762611  3300          0.3751477718 
1.0000000000  0.9290953545  1.0000000000  0.9615384615  0.9348802395  0.9131736527  1.0000000000  0.9375796178  86.227544910  0.0000041022  2.3044762611  3600          0.3730768474 
1.0000000000  0.9290953545  1.0000000000  0.9615384615  0.9348802395  0.9131736527  1.0000000000  0.9375796178  93.413173652  0.0000032570  2.3044762611  3900          0.3755934437 
1.0000000000  0.9290953545  1.0000000000  0.9615384615  0.9356287425  0.9131736527  1.0000000000  0.9375796178  100.59880239  0.0000025292  2.3044762611  4200          0.3804417610 
1.0000000000  0.9290953545  1.0000000000  0.9658119658  0.9356287425  0.9131736527  1.0000000000  0.9363057325  107.78443113  0.0000022135  2.3044762611  4500          0.3776012532 
1.0000000000  0.9290953545  1.0000000000  0.9636752137  0.9363772455  0.9131736527  1.0000000000  0.9363057325  114.97005988  0.0000017496  2.3044762611  4800          0.3697694286 
1.0000000000  0.9290953545  1.0000000000  0.9636752137  0.9341317365  0.9131736527  1.0000000000  0.9375796178  119.76047904  0.0000015477  2.3044762611  5000          0.3733742237 
Environment:
	Python: 3.10.12
	PyTorch: 2.1.0+cu121
	Torchvision: 0.16.0+cu121
	CUDA: 12.1
	CUDNN: 8902
	NumPy: 1.23.5
	PIL: 9.4.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"resnet18": true, "batch_size": 32, "data_augmentation": false}
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: False
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: True
	resnet_dropout: 0.0
	weight_decay: 0.0
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.2092739475  0.2004889976  0.2201492537  0.2200854701  0.2223053892  0.2574850299  0.2127862595  0.2229299363  0.0000000000  2.1099038124  2.2035861015  0             1.2285096645 
0.9969493594  0.9144254279  0.9989339019  0.9294871795  1.0000000000  0.9580838323  0.5419847328  0.5452229299  7.1856287425  0.1507611899  2.3044762611  300           0.3754547898 
1.0000000000  0.9290953545  0.9994669510  0.9380341880  1.0000000000  0.9610778443  0.6272264631  0.6229299363  14.371257485  0.0075870331  2.3044762611  600           0.3739577548 
1.0000000000  0.9388753056  1.0000000000  0.9508547009  1.0000000000  0.9670658683  0.6071882952  0.6127388535  21.556886227  0.0001336963  2.3044762611  900           0.3708871635 
1.0000000000  0.9364303178  1.0000000000  0.9508547009  1.0000000000  0.9670658683  0.6132315522  0.6127388535  28.742514970  0.0000398881  2.3044762611  1200          0.3766344436 
1.0000000000  0.9388753056  1.0000000000  0.9487179487  1.0000000000  0.9640718563  0.6113231552  0.6140127389  35.928143712  0.0000238817  2.3044762611  1500          0.3743999354 
1.0000000000  0.9364303178  1.0000000000  0.9487179487  1.0000000000  0.9640718563  0.6129134860  0.6127388535  43.113772455  0.0000155385  2.3044762611  1800          0.3699059757 
1.0000000000  0.9364303178  1.0000000000  0.9487179487  1.0000000000  0.9610778443  0.6132315522  0.6140127389  50.299401197  0.0000106440  2.3044762611  2100          0.3788259888 
1.0000000000  0.9339853301  1.0000000000  0.9487179487  1.0000000000  0.9610778443  0.6122773537  0.6165605096  57.485029940  0.0000076227  2.3044762611  2400          0.3721778131 
1.0000000000  0.9315403423  1.0000000000  0.9487179487  1.0000000000  0.9610778443  0.6125954198  0.6101910828  64.670658682  0.0000059058  2.3044762611  2700          0.3713266174 
1.0000000000  0.9315403423  1.0000000000  0.9487179487  1.0000000000  0.9580838323  0.6122773537  0.6127388535  71.856287425  0.0000043173  2.3044762611  3000          0.3769061081 
1.0000000000  0.9315403423  1.0000000000  0.9508547009  1.0000000000  0.9580838323  0.6106870229  0.6114649682  79.041916167  0.0000035477  2.3044762611  3300          0.3734067257 
1.0000000000  0.9315403423  1.0000000000  0.9508547009  1.0000000000  0.9610778443  0.6084605598  0.6114649682  86.227544910  0.0000027258  2.3044762611  3600          0.3814370950 
1.0000000000  0.9290953545  1.0000000000  0.9508547009  1.0000000000  0.9580838323  0.6100508906  0.6101910828  93.413173652  0.0000021851  2.3044762611  3900          0.3917873255 
1.0000000000  0.9290953545  1.0000000000  0.9508547009  1.0000000000  0.9580838323  0.6103689567  0.6140127389  100.59880239  0.0000018531  2.3044762611  4200          0.3777563874 
1.0000000000  0.9339853301  1.0000000000  0.9508547009  1.0000000000  0.9580838323  0.6049618321  0.6089171975  107.78443113  0.0000014958  2.3044762611  4500          0.3783051117 
1.0000000000  0.9315403423  1.0000000000  0.9508547009  1.0000000000  0.9580838323  0.6049618321  0.6076433121  114.97005988  0.0000012321  2.3044762611  4800          0.3764989066 
1.0000000000  0.9315403423  1.0000000000  0.9508547009  1.0000000000  0.9610778443  0.6046437659  0.6089171975  119.76047904  0.0000010614  2.3044762611  5000          0.3766083157 
